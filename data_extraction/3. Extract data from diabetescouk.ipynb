{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Using BeautifulSoup to parse and extract data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60ca769b89e1e7b6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ],
   "metadata": {},
   "id": "707d44a9",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "c55ede03",
   "metadata": {},
   "source": [
    "## Extract Title and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a90224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def extract_content(num_pages, base_url):\n",
    "    usernames = []\n",
    "    profile_links = []\n",
    "    post_titles = []\n",
    "    comments = []\n",
    "    contents = []\n",
    "\n",
    "    # set the headers\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "\n",
    "    # loop through the specified number of pages\n",
    "    for page in tqdm(range(1, num_pages + 1)):\n",
    "        # request the page content\n",
    "        url = f\"{base_url}page-{page}\"\n",
    "        time.sleep(5)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # extract the username and profile link\n",
    "        for user in soup.find_all(\"ul\", class_=\"structItem-parts\"):\n",
    "            usernames.append(user.find(\"a\").text.strip())\n",
    "            user_link = user.find(\"a\")[\"href\"]\n",
    "            profile_link = \"https://www.diabetes.co.uk\" + user_link + \"#about\"\n",
    "            profile_links.append(profile_link)\n",
    "\n",
    "        # extract the post title and link to comments\n",
    "        for title in soup.find_all(\"div\", class_=\"structItem-title\"):\n",
    "            title_name = title.find(\"a\", class_=\"\").text.strip()\n",
    "            link = title.find(\"a\")[\"href\"]\n",
    "            post_titles.append(title_name)\n",
    "\n",
    "            # request the comment page and extract the comments\n",
    "            response = requests.get(\"https://www.diabetes.co.uk\" + link, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            comment_list = []\n",
    "            self_content = []\n",
    "            \n",
    "            for blockquote in soup.find_all('blockquote'): # remove quotes\n",
    "                blockquote.extract()\n",
    "                \n",
    "            comments_section = soup.find_all(\"div\", class_=\"bbWrapper\")\n",
    "            \n",
    "            for i, comment in enumerate(comments_section):\n",
    "                if i == 0:\n",
    "                    self_content.append(comment.text.strip())\n",
    "                else:\n",
    "                    comment_list.append(comment.text.strip())\n",
    "            contents.append(self_content)\n",
    "            comments.append(comment_list)\n",
    "\n",
    "    # create a pandas dataframe and save to CSV\n",
    "    df = pd.DataFrame({\n",
    "        \"Username\": usernames,\n",
    "        \"Profile Link\": profile_links,\n",
    "        \"Post Title\": post_titles,\n",
    "        \"Content\": contents,\n",
    "        \"Comments\": comments\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c40046d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 15/15 [03:47<00:00, 15.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Young People/Adults\n",
    "num_pages = 15\n",
    "base_url = \"https://www.diabetes.co.uk/forum/category/young-people-adults.75/\"\n",
    "young_adult = extract_content(num_pages, base_url)\n",
    "young_adult.to_excel(\"young_adult_1.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61522c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 29/29 [07:30<00:00, 15.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Children & Teens\n",
    "num_pages = 29\n",
    "base_url = \"https://www.diabetes.co.uk/forum/category/children-teens.46/\"\n",
    "children_teen = extract_content(num_pages, base_url)\n",
    "children_teen.to_excel('children_teen_1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c938427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 752/752 [3:17:44<00:00, 15.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# Type 1 Diabetes\n",
    "num_pages = 752\n",
    "base_url = \"https://www.diabetes.co.uk/forum/category/type-1-diabetes.19/\"\n",
    "type1diabetes = extract_content(num_pages, base_url)\n",
    "type1diabetes.to_excel('type1diabetes_1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76bd7ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 52/52 [12:52<00:00, 14.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parents\n",
    "num_pages = 52\n",
    "base_url = \"https://www.diabetes.co.uk/forum/category/parents.16/\"\n",
    "parents = extract_content(num_pages, base_url)\n",
    "parents.to_excel('parents_1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3055663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge DataFrame\n",
    "type1diabetes['platform'] = 'type1diabetes'\n",
    "young_adult['platform']= 'young_adult' \n",
    "children_teen['platform'] = 'children_teen' \n",
    "df = pd.concat([type1diabetes, young_adult, children_teen])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57619e94",
   "metadata": {},
   "source": [
    "## Identify Type 1 from young_adult and child_teen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_T2 = pd.concat([young_adult, children_teen])\n",
    "profile_links_diabetes = list(set(T1_T2['Profile Link'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Extract profiles with diabetes information\n",
    "def scrape_diabetes_info(profile_links_diabetes):\n",
    "    # Configure Chrome options for headless mode\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode (no GUI)\n",
    "\n",
    "    diabetes_info_dict = {}\n",
    "\n",
    "    for url in tqdm(profile_links_diabetes):\n",
    "        # Initialize the WebDriver with the specified options for each URL\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        # Load the webpage with Selenium\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for JavaScript to execute (you may need to adjust the wait time)\n",
    "        import time\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        # Get the page source with the modified content (after JavaScript execution)\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Close the Selenium WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "        # Parse the modified page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        # Find the type of diabetes element\n",
    "        diabetes_element = soup.find(\"dt\", text=\"Type of diabetes\")\n",
    "\n",
    "        if diabetes_element:\n",
    "            diabetes_info = diabetes_element.find_next(\"dd\").text.strip()\n",
    "            diabetes_info_dict[url] = diabetes_info\n",
    "        else:\n",
    "            # Store the profile link as the key and \"No Age Information\" as the value\n",
    "            diabetes_info_dict[url] = \"No type of diabetes Information\"\n",
    "\n",
    "    return diabetes_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3802cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_type = scrape_diabetes_info(profile_links_diabetes)\n",
    "df_diabetes_type = pd.DataFrame(diabetes_type.items(), columns=['Profile Link', 'diabetes_type_info'])\n",
    "df_diabetes_type.to_excel('diabetes_type_info.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e0e13428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge diabetes type info\n",
    "T1_T2 = T1_T2.merge(df_diabetes_type, on='Profile Link', how='left')\n",
    "df_t1 = T1_T2[T1_T2['diabetes_type_info']=='Type 1'] # Identify young people with T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ce3f2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_t1, type1diabetes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe83f9",
   "metadata": {},
   "source": [
    "## Extract Age Information and identify young people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e0ab8",
   "metadata": {},
   "source": [
    "BeautifulSoup is parsing the HTML content at the time of the request, and it does not execute JavaScript. It retrieves the original HTML content without any JavaScript modifications. If you want to scrape data from a webpage that relies on JavaScript execution to modify its content, you should use Selenium or another tool capable of executing JavaScript to ensure you get the most up-to-date and modified content.\n",
    "\n",
    "After get all of the profile_links, extract all the age information for the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d3446d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_links = list(set(df['Profile Link'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d63e3700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 6350/6350 [19:32:23<00:00, 11.08s/it]\n"
     ]
    }
   ],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode (no GUI)\n",
    "\n",
    "birthday_info_dict = {}\n",
    "\n",
    "for url in tqdm(profile_links):\n",
    "    # Initialize the WebDriver with the specified options for each URL\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Load the webpage with Selenium\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for JavaScript to execute (you may need to adjust the wait time)\n",
    "    import time\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Get the page source with the modified content (after JavaScript execution)\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Close the Selenium WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Parse the modified page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    # Find the birthday element\n",
    "    birthday_element = soup.find(\"dt\", text=\"Birthday\")\n",
    "\n",
    "    if birthday_element:\n",
    "        # Extract the birthday information from the next sibling (dd) element\n",
    "        birthday_info = birthday_element.find_next(\"dd\").text.strip()\n",
    "        # Store the profile link as the key and age as the value in the dictionary\n",
    "        birthday_info_dict[url] = birthday_info\n",
    "    else:\n",
    "        # Store the profile link as the key and \"No Age Information\" as the value\n",
    "        birthday_info_dict[url] = \"No Age Information\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c18e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_birth = pd.DataFrame(birthday_info_dict.items(), columns=['Profile Link', 'Age_info'])\n",
    "df_birth.to_excel('birthday_info.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "787b4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge age infor\n",
    "merged_df = df.merge(df_birth, how='left', on='Profile Link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a87e71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract age from text\n",
    "def extract_age(text):\n",
    "    if \"No Age Information\" in text:\n",
    "        return np.nan\n",
    "    else:\n",
    "        age_match = re.search(r'Age:\\s*(\\d+)', text)\n",
    "        if age_match:\n",
    "            return int(age_match.group(1))\n",
    "        else:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "11824aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns={'Age':'Age_info'}, inplace=True)\n",
    "merged_df['Age'] = merged_df['Age_info'].apply(extract_age)\n",
    "merged_df.drop(['diabetes_type_info', 'Age_info'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "979c0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel('diabetes.co.uk.xlsx') # corpus with unclear age\n",
    "df_age = merged_df[~merged_df['Age'].isna()] \n",
    "df_age.to_excel('corpus_with_age_info.xlsx') # corpus with exact age "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Identify young people with T1\n",
    "young_df = df_age[(df_age['Age']<27) & (df_age['Age']>12)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28a9727aa85f64be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
